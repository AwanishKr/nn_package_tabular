import math
import torch
import numpy as np
import pandas as pd

from copy import deepcopy
from collections import defaultdict
from sklearn.neighbors import NearestNeighbors



def add_logits_to_aum_dict(model, loader, device, aum_dict_template):
    """
    Collect logits + targets for all samples at the current checkpoint/epoch.
    Appends new logits/targets instead of overwriting.

    Args:
        model: Trained torch model
        loader: PyTorch DataLoader returning (x, y, ..., sample_id)
        device: torch.device
        aum_dict_template: dict with sample_ids as keys (initialized with count/margin/aum etc.)
        batch_size: size of each batch
    """
    model.eval()
    aum_dict_epoch = deepcopy(aum_dict_template)  # copy existing dict

    with torch.no_grad():
        for inputs, targets, _, sample_ids in loader:
            inputs = inputs.to(device)
            targets = targets.to(device)
            logits = model(inputs)  # [B, num_classes]
            
            logits_list = logits.detach().cpu().tolist()
            targets_list = targets.detach().cpu().tolist()
            
            # Convert sample_ids to list if it's a tensor (defensive programming)
            if isinstance(sample_ids, torch.Tensor):
                sample_ids = sample_ids.tolist()

            for sample_id, logit_vec, target in zip(sample_ids, logits_list, targets_list):
                if "logits" not in aum_dict_epoch[sample_id]:
                    aum_dict_epoch[sample_id]["logits"] = []
                if "targets" not in aum_dict_epoch[sample_id]:
                    aum_dict_epoch[sample_id]["targets"] = []

                # append new values for this checkpoint
                aum_dict_epoch[sample_id]["logits"] = logit_vec
                aum_dict_epoch[sample_id]["targets"] = target

    return aum_dict_epoch



def calculate_aum(logits, targets, sample_ids, aum_dict, epoch):
    target_values = logits.gather(1, targets.view(-1, 1)).squeeze()

    # mask out target values
    masked_logits = torch.scatter(logits, 1, targets.view(-1, 1), float('-inf'))
    other_logit_values, _ = masked_logits.max(1)
    other_logit_values = other_logit_values.squeeze()
    margin_values = (target_values - other_logit_values).tolist()

    for sample_id, margin in zip(sample_ids, margin_values):
        if sample_id not in aum_dict:
            aum_dict[sample_id] = {
                "epoch": epoch,
                "margin_sum": margin,
                "aum": margin
            }
        if "epoch" not in aum_dict[sample_id]:  # first time seeing this sample
            aum_dict[sample_id] = {
                "epoch": epoch,
                "margin_sum": margin,
                "aum": margin
            }
        else:
            aum_dict[sample_id]["epoch"] = epoch
            aum_dict[sample_id]["margin_sum"] += margin
            aum_dict[sample_id]["aum"] = (
                aum_dict[sample_id]["margin_sum"] / aum_dict[sample_id]["epoch"]
            )

    return aum_dict


import torch
from collections import defaultdict


def compute_grand_score(model, X_batch, y_batch, criterion, grand_scores, sample_ids):
    """
    GRAND computation:
    - uses per-sample loss (criterion with reduction='none')
    - computes gradients with torch.autograd.grad
    - maintains {id: [count, sum, mean]} dict
    """
    model.eval()  # disable dropout/bn randomness

    X_batch, y_batch = X_batch.to(next(model.parameters()).device), y_batch.to(next(model.parameters()).device)

    # Ensure gradients are enabled
    with torch.set_grad_enabled(True):
        outputs = model(X_batch)
        losses = criterion(outputs, y_batch)  # shape [B], still connected to graph

        grad_norms = []
        for i in range(X_batch.size(0)):
            grads = torch.autograd.grad(
                losses[i],
                model.parameters(),
                retain_graph=True,
                create_graph=False
            )
            grad_norm = torch.cat([g.reshape(-1) for g in grads]).norm().item()
            grad_norms.append(grad_norm)

    # update dict {id: [count, sum, mean]}
    for sample_id, gn in zip(sample_ids, grad_norms):
        if len(grand_scores[sample_id]) == 0:
            grand_scores[sample_id] = {
                "count": 1,
                "gn_sum": gn,
                "gn_mean": gn
            }
            # grand_scores[sample_id].extend([1, gn, gn])  # count, sum, mean
        else:
            grand_scores[sample_id]["count"] += 1
            grand_scores[sample_id]["gn_sum"] += gn
            grand_scores[sample_id]["gn_mean"] = grand_scores[sample_id][1] / grand_scores[sample_id][0]
            

    return grand_scores


            
    
def EL2N_score(probs, y, sample_ids, el2n_scores, epoch):
    """
    Compute/update EL2N scores.
    
    probs: [B, C] softmax probabilities
    y:     [B] ground-truth labels
    sample_ids: list/array of IDs for the batch
    el2n_scores: dict {id: [count, sum, mean]} to be updated
    """
    one_hot = torch.nn.functional.one_hot(y, num_classes=probs.size(1)).float()
    errors = torch.norm(probs - one_hot, dim=1)   # [B]

    for sid, err in zip(sample_ids, errors):
        err_val = err.item()
        if sid not in el2n_scores:
            el2n_scores[sid] = {
                "epoch": epoch,
                "sum": err_val,
                "mean": err_val,
            }
        elif len(el2n_scores[sid]) == 0:
            el2n_scores[sid].extend([epoch, err_val, err_val])  # count, sum, mean
        else:
            el2n_scores[sid]['epoch'] = epoch
            el2n_scores[sid]['sum'] += err_val
            el2n_scores[sid]['mean'] = el2n_scores[sid]['sum'] / el2n_scores[sid]['epoch']

    return el2n_scores
        
            
            
def update_forgetting(logits, labels, sample_ids, epoch, forgetting_scores):
    """
    Update forgetting statistics.

    forgetting_scores[sid] = {
        "forget_count": int,
        "first_learned": int or None,
        "last_learn_epoch": int or None,
        "correct_count": int,
        "first_forget_epoch": int or None
    }
    """
    preds = logits.argmax(dim=1)

    for pred, label, sid in zip(preds, labels, sample_ids):
        # Initialize entry if first time
        if sid not in forgetting_scores:
            forgetting_scores[sid] = {
                "forget_count": 0,
                "first_learned": None,
                "last_learn_epoch": None,
                "correct_count": 0,
                "first_forget_epoch": None,
                "_last_correct": False  # internal tracker, will be deleted later
            }

        entry = forgetting_scores[sid]
        was_correct = entry["_last_correct"]
        now_correct = (pred.item() == label.item())

        # Count correct predictions
        if now_correct:
            entry["correct_count"] += 1
            if entry["first_learned"] is None:
                entry["first_learned"] = epoch
            if not was_correct:  # flip from wrong â†’ correct
                entry["last_learn_epoch"] = epoch

        # Forgetting event
        if was_correct and not now_correct:
            entry["forget_count"] += 1
            if entry["first_forget_epoch"] is None:
                entry["first_forget_epoch"] = epoch

        # Update last correctness
        entry["_last_correct"] = now_correct

    return forgetting_scores





from sklearn.neighbors import NearestNeighbors
import torch
import numpy as np

def prediction_depth_knn(layer_reps, y, sample_ids, depth_scores, k=10):
    """
    layer_reps: dict[layer_idx -> tensor [N, D]] representations
    y: [N] ground-truth labels (long tensor)
    sample_ids: list of IDs
    depth_scores: dict {id: [depth]} to be updated
    k: neighbors
    
    Returns: updated depth_scores
    """
    N = y.size(0)
    y_np = y.cpu().numpy()

    # Init
    for sid in sample_ids:
        if sid not in depth_scores:
            depth_scores[sid] = [float('inf')]

    for layer_idx, X in layer_reps.items():
        X_np = X.detach().cpu().numpy()

        nn = NearestNeighbors(n_neighbors=k+1, metric="euclidean").fit(X_np)
        _, neigh_idx = nn.kneighbors(X_np)

        for i, sid in enumerate(sample_ids):
            neighbors = neigh_idx[i][1:]  # exclude self
            maj_vote = np.bincount(y_np[neighbors]).argmax()
            if depth_scores[sid][0] == float('inf') and maj_vote == y_np[i]:
                depth_scores[sid][0] = layer_idx

    return depth_scores


